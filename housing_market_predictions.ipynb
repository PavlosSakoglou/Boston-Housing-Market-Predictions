{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Boston Housing Market Project\n",
    "\n",
    "Machine Learning NanoDegree - Udacity\n",
    "\n",
    "Pavlos Sakoglou \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "import visuals as vs\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "data = pd.read_csv('housing.csv')\n",
    "prices = data['MEDV']\n",
    "features = data.drop('MEDV', axis = 1)\n",
    "\n",
    "print(prices)\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "# TODO: Minimum price of the data\n",
    "minimum_price = np.min(prices)\n",
    "\n",
    "# TODO: Maximum price of the data\n",
    "maximum_price = np.max(prices)\n",
    "\n",
    "# TODO: Mean price of the data\n",
    "mean_price = np.mean(prices)\n",
    "\n",
    "# TODO: Median price of the data\n",
    "median_price = np.median(prices)\n",
    "\n",
    "# TODO: Standard deviation of prices of the data\n",
    "std_price = np.std(prices)\n",
    "\n",
    "# Show the calculated statistics\n",
    "print(\"\\nStatistics for Boston housing dataset:\\n\")\n",
    "print(\"Minimum price: ${:,.2f}\".format(minimum_price))\n",
    "print(\"Maximum price: ${:,.2f}\".format(maximum_price))\n",
    "print(\"Mean price: ${:,.2f}\".format(mean_price))\n",
    "print(\"Median price ${:,.2f}\".format(median_price))\n",
    "print(\"Standard deviation of prices: ${:,.2f}\".format(std_price))\n",
    "\n",
    "\n",
    "''' \n",
    "Question 1:\n",
    "    Using your intuition, for each of the three features above, do you think \n",
    "    that an increase in the value of that feature would lead to an increase in \n",
    "    the value of 'MEDV' or a decrease in the value of 'MEDV'? Justify your \n",
    "    answer for each.\n",
    "    \n",
    "Answer:\n",
    "    Let's examine things carefully:\n",
    "        \n",
    "    Since these features reflect the quality and level of life in these neighborhoods,\n",
    "    it is natural to assume that possible increases or decreases in value will determine\n",
    "    the demand of the market over the houses, which will reflect in higher or lower\n",
    "    prices. \n",
    "        \n",
    "    An increase of the values of LSTAT feature, which represents the percentage \n",
    "    of lower class people living in a neighborhood, won't be approaching a \n",
    "    \"better neighborhood\" value, and as a result an increase of this feature more \n",
    "    likely will decrease the value of the house prices, since wealthy people, who \n",
    "    are willing to pay more, want to live in wealthier areas. \n",
    "      \n",
    "    An increase in RM i.e. if the average number of rooms per house increase, \n",
    "    which means if the size of the houses is getting bigger, will imply that the \n",
    "    neighborhood is wealthier, since large houses are found in wealthier areas. \n",
    "    That means, that the prices will also go up, as the size of the houses is \n",
    "    going up. \n",
    "    \n",
    "    For the PTRATIO, if its values increase, it means that there are many students\n",
    "    per teacher, which means that on average the level of education is decreased. \n",
    "    Prestigious schools are famous for having small size classes so that the students\n",
    "    will get more individual attention by the teacher and thus comprehend the\n",
    "    material better. Non prestigious schools, like most public schools, are famous \n",
    "    (or infamous) for the opposite: large size classrooms, prioritizing quantity \n",
    "    over quality. Thus, increased PTRATIO values means the level of education \n",
    "    is decreasing, which means that wealthy potential buyers won't be much interested\n",
    "    in such neighborhoods cause they want their kids to go to good schools, \n",
    "    which in turn will decrease the demand, and as a result the prices of the \n",
    "    houses in these neighborhoods. \n",
    "\n",
    "    In summary:\n",
    "        \n",
    "        As RM increases -> MEDV increases\n",
    "        As LSTAT increases -> MEDV decreases\n",
    "        As PTRATION increases -> MEDV decreases\n",
    "'''\n",
    "\n",
    "# TODO: Import 'r2_score'\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def performance_metric(y_true, y_predict):\n",
    "    \"\"\" Calculates and returns the performance score between \n",
    "        true and predicted values based on the metric chosen. \"\"\"\n",
    "    \n",
    "    # TODO: Calculate the performance score between 'y_true' and 'y_predict'\n",
    "    score = r2_score(y_true, y_predict)\n",
    "    \n",
    "    # Return the score\n",
    "    return score\n",
    "\n",
    "# Calculate the performance of this model\n",
    "score = performance_metric([3, -0.5, 2, 7, 4.2], [2.5, 0.0, 2.1, 7.8, 5.3])\n",
    "print(\"\\nModel has a coefficient of determination, R^2, of {:.3f}.\".format(score))\n",
    "\n",
    "''' \n",
    "Question 2:\n",
    "    Would you consider this model to have successfully captured the variation of the target variable?\n",
    "    Why or why not?\n",
    "\n",
    "Answer:\n",
    "    Yes. The output of the above metric given the data above is 0.923, which is in a  \n",
    "    neighborhood of 1 with radius = 0.077. Sounds pretty good score given that\n",
    "    a score of 1 perfectly predicts the target variable. \n",
    "    \n",
    "'''\n",
    "\n",
    "# TODO: Import 'train_test_split'\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# TODO: Shuffle and split the data into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, prices, test_size=20, random_state=True)\n",
    "\n",
    "# Success\n",
    "print(\"Training and testing split was successful.\")\n",
    "\n",
    "\n",
    "'''\n",
    "Question 3:\n",
    "    What is the benefit to splitting a dataset into some ratio of training and \n",
    "    testing subsets for a learning algorithm?\n",
    "    \n",
    "Answer:\n",
    "    It is useful to split the data into training and testing subsets because that\n",
    "    way we can use some of them (most of them usually) to train the model, and then \n",
    "    use the rest of the data to test the model. That enables us to extract information\n",
    "    about the model, how well it performs, if it overfits or underfits the data etc. \n",
    "    \n",
    "    If we don't split the data and use all of them for training, then we won't \n",
    "    have any way of knowing how the model might perform in real data. This is a\n",
    "    good technique to help us evaluate our model choices. \n",
    "    \n",
    "    (This is from lesson 6: testing models)\n",
    "\n",
    "'''\n",
    "\n",
    "# Produce learning curves for varying training set sizes and maximum depths\n",
    "vs.ModelLearning(features, prices)\n",
    "\n",
    "'''\n",
    "Question 4:\n",
    "    Choose one of the graphs above and state the maximum depth for the model.\n",
    "    What happens to the score of the training curve as more training points are added? \n",
    "    What about the testing curve?\n",
    "    Would having more training points benefit the model?\n",
    "    \n",
    "Answer:\n",
    "    I am choosing the second curve with max_depth = 3, which is highly biased. Deeper-depth\n",
    "    trees give better accuracy on the training set, and less deep-depth trees give\n",
    "    better accuracy on the testing set. Moreover, deep trees let the model overfit\n",
    "    the data and create a closely fit decision boundary which often does not\n",
    "    correspond to the actual boundary.\n",
    "    \n",
    "    With:\n",
    "        max_depth: 3\n",
    "        Testing score: blue curve\n",
    "        Training score: red curve\n",
    "    \n",
    "    We have the following conclusions:\n",
    "        1) The score of training curve decreases the more we add training points\n",
    "            and at some point stabilizes -- it's still high\n",
    "        2) The score of testing curve increases the more we add training points\n",
    "           and at some point stabilizes -- it's still high\n",
    "          \n",
    "    The more training points we have, the lesser the training score will be and the \n",
    "    higher the testing score will be until they converge, which implies a highly \n",
    "    biased model. \n",
    "\n",
    "'''\n",
    "\n",
    "vs.ModelComplexity(X_train, y_train)\n",
    "\n",
    "'''\n",
    "Question 5:\n",
    "    When the model is trained with a maximum depth of 1, does the model suffer \n",
    "    from high bias or from high variance?\n",
    "    How about when the model is trained with a maximum depth of 10? \n",
    "    What visual cues in the graph justify your conclusions?\n",
    "\n",
    "Answer:\n",
    "    When the model is trained with a max depth of 1, then it suffers from high bias\n",
    "    When the model is trained with a max depth of 10, then it suffers from high variance.\n",
    "    The above statements are obvious from the curves, by observing how they behave\n",
    "    as # of depth -> inf. \n",
    "    The more the # of depth grows, the training score is pretty high, while the \n",
    "    validation score decreases. That means that as the # of depth increases, the more\n",
    "    our model \"memorizes\" the data instead of learning from them, which makes the validation\n",
    "    curve to decrease, cause the model won't validate as good with high bias.\n",
    "\n",
    "\n",
    "Question 6: \n",
    "    Which maximum depth do you think results in a model that best generalizes to unseen data?\n",
    "    What intuition lead you to this answer?\n",
    "\n",
    "Answer:\n",
    "    Ideally we would want our model to have a good trade off between variance and bias.\n",
    "    That is, we don't want our model to neither overfit (high variance) nor underfit\n",
    "    (high bias). To achieve that, I believe that the max_depth should be 3 or 4.\n",
    "    \n",
    "    I arrived to this conclusion by looking at the graphs. The first graph with the \n",
    "    4 plots shows us that a good fit and trade off is achieved with depth 3 versus\n",
    "    the other graphs with increased depth. Then the second graph with the validation \n",
    "    curve shows that the validation score is in an acceptable region between max depth\n",
    "    3 and 4.  \n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "Question 7:\n",
    "    What is the grid search technique?\n",
    "    How it can be applied to optimize a learning algorithm?\n",
    "    \n",
    "Answer:\n",
    "    The grid search technique is used when we have a set of models to choose from, \n",
    "    which differ from each other in their parameter values that lie on a gird.\n",
    "    We then train each model and evaluate them using cross-validation and select \n",
    "    the one that performed best. \n",
    "    \n",
    "    Further research in wikipedia (https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)):\n",
    "        \n",
    "        Hyperparameter optimization is the problem of choosing a set of optimal \n",
    "        hyperparameters for a learning algorithm.\n",
    "        \n",
    "        The traditional way of performing hyperparameter optimization is grid search,\n",
    "        which is simply an exhaustive searching through a manually specified subset \n",
    "        of the hyperparameter space of a learning algorithm. \n",
    "        \n",
    "        A grid search algorithm must be guided by some performance metric, \n",
    "        typically measured by cross-validation on the training set or evaluation \n",
    "        on a held-out validation set.\n",
    "    \n",
    "    \n",
    "\n",
    "Question 8:\n",
    "    What is the k-fold cross-validation training technique?\n",
    "    What benefit does this technique provide for grid search when optimizing a model?\n",
    "\n",
    "Answer:\n",
    "    \n",
    "    K-fold validation is when we split our data into K subsets and use each subset\n",
    "    separately as testing data, and the rest points as training data. We then train \n",
    "    our model K different times, one for each subset, and then we average the results\n",
    "    to get the final model. \n",
    "    \n",
    "    To do grid search, iterate through a wide range of grid combinations. \n",
    "    Then, choose the set of parameters for which k-folds reports the lowest error.            \n",
    "    \n",
    "'''\n",
    "\n",
    "\n",
    "# TODO: Import 'make_scorer', 'DecisionTreeRegressor', and 'GridSearchCV'\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "def fit_model(X, y):\n",
    "    \"\"\" Performs grid search over the 'max_depth' parameter for a \n",
    "        decision tree regressor trained on the input data [X, y]. \"\"\"\n",
    "    \n",
    "    # Create cross-validation sets from the training data\n",
    "    # sklearn version 0.18: ShuffleSplit(n_splits=10, test_size=0.1, train_size=None, random_state=None)\n",
    "    # sklearn versiin 0.17: ShuffleSplit(n, n_iter=10, test_size=0.1, train_size=None, random_state=None)\n",
    "    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n",
    "  \n",
    "    # TODO: Create a decision tree regressor object\n",
    "    regressor = DecisionTreeRegressor(random_state = 0)\n",
    "\n",
    "    # TODO: Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n",
    "    params = {\"max_depth\" : [1,2,3,4,5,6,7,8,9,10]}\n",
    "\n",
    "    # TODO: Transform 'performance_metric' into a scoring function using 'make_scorer' \n",
    "    scoring_fnc = make_scorer(performance_metric)\n",
    "\n",
    "    # TODO: Create the grid search cv object --> GridSearchCV()\n",
    "    # Make sure to include the right parameters in the object:\n",
    "    # (estimator, param_grid, scoring, cv) which have values 'regressor', 'params', 'scoring_fnc', and 'cv_sets' respectively.\n",
    "    grid = GridSearchCV(regressor, params, scoring_fnc, cv=cv_sets)\n",
    "\n",
    "    # Fit the grid search object to the data to compute the optimal model\n",
    "    grid = grid.fit(X, y)\n",
    "\n",
    "    # Return the optimal model after fitting the data\n",
    "    return grid.best_estimator_\n",
    "\n",
    "# Fit the training data to the model using grid search\n",
    "reg = fit_model(X_train, y_train)\n",
    "\n",
    "# Produce the value for 'max_depth'\n",
    "print(\"Parameter 'max_depth' is {} for the optimal model.\".format(reg.get_params()['max_depth']))\n",
    "\n",
    "'''\n",
    "Question 9:\n",
    "    What maximum depth does the optimal model have? \n",
    "    How does this result compare to your guess in Question 6?\n",
    "    \n",
    "Answer:\n",
    "    \"Parameter 'max_depth' is 4 for the optimal model.\"\n",
    "    \n",
    "    In Question 6, by looking at the plots I assumed a max depth of 3 or 4. \n",
    "    Apparently there is another correlation here that I missed. If you look at \n",
    "    the second graph, the validation reaches a maximum value at depth 4. If this is\n",
    "    true, then our job becomes easier to quickly get an idea of how the max depth would\n",
    "    look like for our model.    \n",
    "\n",
    "'''\n",
    "\n",
    "# Produce a matrix for client data\n",
    "client_data = [[5, 17, 15], # Client 1\n",
    "               [4, 32, 22], # Client 2\n",
    "               [8, 3, 12]]  # Client 3\n",
    "\n",
    "# Show predictions\n",
    "for i, price in enumerate(reg.predict(client_data)):\n",
    "    print (\"Predicted selling price for Client {}'s home: ${:,.2f}\".format(i+1, price))\n",
    "\n",
    "\n",
    "'''\n",
    "Question 10:\n",
    "    What price would you recommend each client sell his/her home at?\n",
    "    Do these prices seem reasonable given the values for the respective features?\n",
    "    \n",
    "Answer:\n",
    "    As per the model's output:\n",
    "        Client 1 should sell at: $408,800.00\n",
    "        Client 2 should sell at: $231,675.00\n",
    "        Client 3 should sell at: $931,350.00\n",
    "    \n",
    "    Client 1 has a 5-bedroom house (which is large enough), a decent poverty level\n",
    "    at 17%, and a good student/teacher ratio. We can't draw any conclusions by solely\n",
    "    examining this client. So let's look at the other two.\n",
    "    \n",
    "    Client 2 has a 4-room house (which is relatively smaller), higher poverty rate, \n",
    "    almost double in fact, and a border-line average student/teacher ratio. Without\n",
    "    looking at the predicted price, We should expect it to be lower since all the \n",
    "    characteristics are worse than Client 1's characteristics. \n",
    "    Indeed the price makes sense to be smaller at $231,675\n",
    "    \n",
    "    Client 3 has a significantly larger house (8 rooms is a palace!), almost no lower\n",
    "    class population (3% poverty), and a great student/teacher ratio, much better\n",
    "    than both of the other clients. Thus, we expect the price to be much higher. \n",
    "    Indeed the predicted price is $931,350, which is more than double from Client 1.\n",
    "    I would personally expect it to be around double, but I guess the poverty rate \n",
    "    which itself is almost 6 times less than Client 1's neighborhood makes the difference, \n",
    "    along with the extra 3 bedrooms. \n",
    "    \n",
    "'''\n",
    "\n",
    "vs.PredictTrials(features, prices, fit_model, client_data)\n",
    "\n",
    "'''\n",
    "Question 11:\n",
    "    In a few sentences, discuss whether the constructed model should or \n",
    "    should not be used in a real-world setting.\n",
    "    \n",
    "    In particular:\n",
    "        How relevant today is data that was collected from 1978? How important is inflation?\n",
    "        \n",
    "        Are the features present in the data sufficient to describe a home? \n",
    "        Do you think factors like quality of apppliances in the home, square \n",
    "        feet of the plot area, presence of pool or not etc should factor in?\n",
    "        \n",
    "        Is the model robust enough to make consistent predictions?\n",
    "        \n",
    "        Would data collected in an urban city like Boston be applicable in a rural city?\n",
    "        \n",
    "        Is it fair to judge the price of an individual home based on the characteristics \n",
    "        of the entire neighborhood?\n",
    "        \n",
    "\n",
    "Answer:\n",
    "    \n",
    "    Data collected in 1978 are actually pretty old. Many things happened ever since, \n",
    "    the inflation increased by 276.6%, city projects, perhaps extreme natural conditions\n",
    "    (snowstorms, floods, etc.) that might have changed the preferences of the people.\n",
    "    Nevertheless, one can argue that the prices in the housing market remained analogous\n",
    "    to those at the time. It could be true that the prices only increased percentage-wise\n",
    "    thus not being affected much by all these events, but only their value to have changed. \n",
    "    \n",
    "    \n",
    "    I think the data are reflecting the house prices, however there might be some\n",
    "    valuable input ignored. For example, the age of the property, its condition,\n",
    "    if there is an open space etc matter a lot in the price. Although we want fewer\n",
    "    and as most descriptive factors as possible, a couple more factors would be great. \n",
    "    On the other hand, the current factors are still pretty descriptive if we admit that\n",
    "    there is a positive correlation between lower class population and higher student/teacher\n",
    "    ration and criminality. That makes a \"criminality rate\" factor redundant, for instance. \n",
    "    \n",
    "    The model's output is quite reasonable, although not very robust. In particular, \n",
    "    compared to the max price value we computed above, it has approximately 8% of it. \n",
    "    Ideally I would like it to be around 3%-4% or less. \n",
    "    \n",
    "    Depending on the size of the neighborhood, it is reasonable to estimate a range\n",
    "    in which the price of an individual house should be in. We can then classify (cluster)\n",
    "    house prices per basic neighborhood characteristics, but then it would be very bold\n",
    "    to predict the price of an individual house, there it's likely this house to be an \n",
    "    exception (outlier).\n",
    "\n",
    "    In general, if I was to sell such software to a real estate agent I would ask for\n",
    "    more recent data -- data from the previous 5 year up to today. Then I will try to \n",
    "    work a bit more on the rebustness of it, and perhaps compute more metrics to make sure\n",
    "    all is good. Finally I would test it for a month or two, and then deliver it.    \n",
    "    \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
